<html>
<head>
<title>Computer Vision Project 5 Report</title>
<link href='http://fonts.googleapis.com/css?family=Nunito:300|Crimson+Text|Droid+Sans+Mono' rel='stylesheet' type='text/css'>
<link rel="stylesheet" title="Default" href="styles/github.css">
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js"></script>  

<link rel="stylesheet" href="highlighting/styles/default.css">
<script src="highlighting/highlight.pack.js"></script>

<style type="text/css">
body {
	margin: 0px;
	width: 100%;
	font-family: 'Crimson Text', serif;
	font-size: 20px;
	background: #fcfcfc;
}
h1 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 28px;
	margin: 25px 0px 0px 0px;
}

h2 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 32px;
	margin: 15px 0px 35px 0px;
	color: #333;	
	word-spacing: 3px;
}

h3 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 26px;
	margin: 10px 0px 10px 0px;
	color: #333;
	word-spacing: 2px;
}
h4 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 22px;
	margin: 10px 0px 10px 0px;
	color: #333;
	word-spacing: 2px;
}

h5 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 18px;
	margin: 10px 0px 10px 0px;
	color: #111;
	word-spacing: 2px;
}

p, li {
	color: #444;
}

a {
	color: #DE3737;
}

.container {
	margin: 0px auto 0px auto;
	width: 1160px;
}

#header {
	background: #333;
	width: 100%;
}

#headersub {
	color: #ccc;
	width: 960px;
	margin: 0px auto 0px auto;
	padding: 20px 0px 20px 0px;
}

.chart {
	width: 480px;
}
.lol {
	font-size: 16px;
	color: #888;
	font-style: italic;
}
.sep {
	height: 1px;
	width: 100%;
	background: #999;
	margin: 20px 0px 20px 0px;
}
.footer{
	font-size: 16px;
}
.latex {
	width: 100%;
}

.latex img {
	display: block;
	margin: 0px auto 0px auto;
}

pre {
	font-family: 'Droid Sans Mono';
	font-size: 14px;
}

table td {
  text-align: center;
  vertical-align: middle;
}

table td img {
  text-align: center;
  vertical-align: middle;
}

#contents a {
}
</style>
<script type="text/javascript">
    hljs.initHighlightingOnLoad();
</script>
</head>
<body>
<div id="header" >
	<div id="headersub">
		<h1><span style="color: #DE3737">Dibyendu Mondal</span></h1>
	</div>
</div>
<div class="container">

	<h2> Project 5: Face Detection with a Sliding Window</h2>
	<div style="clear:both">
		<p>The goal of this project is to implement a siding window face detector. The sliding window model is conceptually simple: independently classify all image patches as being object or non-object. Sliding window classification is the dominant paradigm in object detection and for one object category in particular -- faces -- it is one of the most noticeable successes of computer vision.
		It involves the following steps:</p>
		<ol>
			<li>Load cropped positive trained examples</li>
			<li>Sample random negative examples</li>
			<li>Train a classifier from these examples</li>
			<li>Run the classifier on the test set</li>
		</ol>
		<h3>Part 1: Load cropped positive trained examples</h3>
		<p>This function returns all positive training examples (faces) from 36x36 images in 'train_path_pos'. Each face is converted into a HoG template according to 'feature_params'.</p>

		<pre><code>
		for i = 1:num_images
			img = im2single(imread(fullfile(train_path_pos,image_files(i).name)));
			features_pos(i,:) = reshape(vl_hog(img,feature_params.hog_cell_size),[1,D]);
		end
		</code></pre>
		<h3>Part 2: Sample random negative examples</h3>
		<p>This function returns negative training examples (non-faces) from any images in 'non_face_scn_path'. Images are converted to grayscale because the positive training data is only available in grayscale.</p>

		<pre><code>
		for i = 1:num_images
			img = im2single(rgb2gray(imread(fullfile(non_face_scn_path,image_files(i).name))));
			x = size(img,2) - feature_params.template_size;
			y = size(img,1) - feature_params.template_size;
			sample_num = min([n,x,y]);
			sample_x = randsample(x,sample_num);
			sample_y = randsample(y,sample_num);
			for j = 1:sample_num
				patch = img(sample_y(j):sample_y(j)+feature_params.template_size-1,sample_x(j):sample_x(j)+feature_params.template_size-1);
				features_neg((i-1)*n+j,:) = reshape(vl_hog(patch,feature_params.hog_cell_size),[1,D]);
			end
		end
		</code></pre>
		<h3>Part 3: Train a classifier from these examples</h3>
		<p>This function calls vl_svmtrain on the returned features. I set lambda as 0.0001.</p>

		<pre><code>
		X = cat(1,features_pos,features_neg);
		Y = cat(1,ones(size(features_pos,1),1),-1*ones(size(features_neg,1),1));
		lambda = 0.0001;
		[w,b] = vl_svmtrain(X',Y',lambda);
		</code></pre>
		<h3>Part 4: Run the classifier on the test set</h3>
		<p>This function returns detections on all of the images in a given path. I convert each test image to HoG feature space with a _single_ call to vl_hog for each scale. Then step over the HoG cells, taking groups of cells that are the same size as the learned template, and classifying them. If the classification is above some confidence, keep the detection and then pass all the detections for an image to non-maximum suppression.</p>

		<pre><code>
		for s = 1:length(scales)
	        scale_img = imresize(img,scales(s));
	        hog_feat = vl_hog(scale_img,feature_params.hog_cell_size);
	        for j = 1:size(hog_feat,1)-n
	            for k = 1:size(hog_feat,2)-n
	                temp_hog_feat = reshape(hog_feat(j:j+n-1,k:k+n-1,:),[1,D]);
	                score = temp_hog_feat*w + b;
	                if score > threshold
	                    y_min = (j-1)*feature_params.hog_cell_size;
	                    x_min = (k-1)*feature_params.hog_cell_size;
	                    y_max = y_min + feature_params.template_size - 1;
	                    x_max = x_min + feature_params.template_size - 1;
	                    y_min = floor(y_min/scales(s)) + 1;
	                    x_min = floor(x_min/scales(s)) + 1;
	                    y_max = floor(y_max/scales(s)) + 1;
	                    x_max = floor(x_max/scales(s)) + 1;
	                    cur_x_min = [cur_x_min;x_min];
	                    cur_y_min = [cur_y_min;y_min];
	                    cur_x_max = [cur_x_max;x_max];
	                    cur_y_max = [cur_y_max;y_max];
	                    cur_confidences = [cur_confidences;score];
	                end
	            end
	        end
	    end
	    cur_bboxes = [cur_x_min,cur_y_min,cur_x_max,cur_y_max];
	    cur_image_ids(1:size(cur_bboxes,1),1) = {test_scenes(i).name};
		</code></pre>
		<h3>Results</h3>
		<div align="center">
			<img src="hog_6/hog_template.png"/><br>
			<img src="hog_6/average_precision.png"/>
			<img src="hog_6/viola-jones.png"/>
			<p style="font-size: 24px">Curves for HOG cell size = 6</p>
		</div>
		<p>We get an average precision of 82.5%. The values of the free parameters are: hog_cell_size = 6, threshold = 0.2 and lambda = 0.0001. The runtime is ~10 mins.</p>
		<div align="center">
			<img src="hog_3/hog_template.png"/><br>
			<img src="hog_3/average_precision.png"/>
			<img src="hog_3/viola-jones.png"/>
			<p style="font-size: 24px">Curves for HOG cell size = 3</p>
		</div>
		<p>We get an average precision of 88.9%. The values of the free parameters are: hog_cell_size = 3, threshold = 0.2 and lambda = 0.0001.</p>
	</div>
</div>
<div align="center">
	<h4>Results on class images</h4>
	<img src="hog_3/detections_4476_2017_class_easy.jpg.png"/>
	<img src="hog_3/detections_4476_2017_class_hard.jpg.png"/>
	<img src="hog_3/detections_4476_2016_class_easy.jpg.png"/>
	<img src="hog_3/detections_4476_2016_class_hard.jpg.png"/>
</div>
<div align="center">
	<h4>Results on other images</h4>
	<img src="hog_3/detections_class57.jpg.png"/>
	<img src="hog_3/detections_Argentina.jpg.png"/>
	<img src="hog_3/detections_albert.jpg.png"/>
	<img src="hog_3/detections_bttf301.jpg.png"/>
</div>
<div class="container">
	<div style="clear:both">
		<h3>Extra Credit: Hard Mining</h3>
		<p>This function returns negative features which have confidence greater than a threshold based on the weight and bias found using the SVM classifier. These negative features are then added to the original negative features list and the weight and bias are re-calculated.</p>

		<pre><code>
		for i = 1:num_images
			img = im2single(rgb2gray(imread(fullfile(non_face_scn_path,image_files(i).name))));
		    if index > 5000
				break;
		    end
		    for s = 1:length(scales)
				scale_img = imresize(img,scales(s));
		        hog_feat = vl_hog(scale_img,feature_params.hog_cell_size);
		        for j = 1:size(hog_feat,1)-n
		            for k = 1:size(hog_feat,2)-n
		                temp_hog_feat = reshape(hog_feat(j:j+n-1,k:k+n-1,:),[1,D]);
		                score = temp_hog_feat*w + b;
		                if score > threshold && index <= 5000
		                	features_neg(index,:) = temp_hog_feat;
		                	index = index + 1;
		                end
		            end
		        end
		    end
		end
		</code></pre>
		<h3>Results</h3>
		<div align="center">
			<img src="hardMining/hog_template.png"/>
			<img src="hardMining/average_precision.png"/>
			<!-- <img src="hardMining/viola-jones.png"/> -->
			<p style="font-size: 24px">Curves for HOG cell size = 3</p>
		</div>
		<p>We get an average precision of 65.9%. The values of the free parameters are: hog_cell_size = 3, threshold = 0.2 and lambda = 0.0001. Learning the hard negatives help us exclude false positives, at the same time it also rejects some true positives. Since the accuracy computation provided in the template code doesn't penalize false positives, the accuracy tends to be lower. Hard negative mining would probably be more important if we had a strict budget of negative training examples or a more expressive, non-linear classifier that can benefit from more trianing data.</p>
	</div>
</div>
<div align="center">
	<h4>Results on some images</h4>
	<img src="hardMining/detections_addams-family.jpg.png"/>
	<img src="hardMining/detections_Argentina.jpg.png"/>
	<img src="hardMining/detections_England.jpg.png"/>
	<img src="hardMining/detections_aeon1a.jpg.png"/>
</div>
<div class="container">
	<div style="clear:both">
		<h3>Extra Credit: Alternative positive training data</h3>
		<p>I modified <i>get_positive_features()</i> so that it includes feautres of an image and the flipped image too.</p>

		<pre><code>
		for i = 1:num_images
			img = im2single(imread(fullfile(train_path_pos,image_files(i).name)));
			features_pos(2*i-1,:) = reshape(vl_hog(img,feature_params.hog_cell_size),[1,D]);
			flip_img = fliplr(img);
			features_pos(2*i,:) = reshape(vl_hog(flip_img,feature_params.hog_cell_size),[1,D]);
		end
		</code></pre>
		<h3>Results</h3>
		<div align="center">
			<img src="flip_pos/hog_template.png"/>
			<img src="flip_pos/average_precision.png"/>
			<img src="flip_pos/viola-jones.png"/>
			<p style="font-size: 24px">Curves for HOG cell size = 3</p>
		</div>
		<p>We get an average precision of 89.4%. The values of the free parameters are: hog_cell_size = 3, threshold = 0.2 and lambda = 0.0001. We observe that on increasing the positive features by including the flip of the images along with the original images increases the accuracy.</p>
	</div>
</div>
<div align="center">
	<h4>Results on some images</h4>
	<img src="flip_pos/detections_addams-family.jpg.png"/>
	<img src="flip_pos/detections_Argentina.jpg.png"/>
	<img src="flip_pos/detections_England.jpg.png"/>
	<img src="flip_pos/detections_aeon1a.jpg.png"/>
</div>
</body>
</html>
